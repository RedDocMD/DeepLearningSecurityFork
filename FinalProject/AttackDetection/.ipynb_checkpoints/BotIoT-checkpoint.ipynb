{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aad9456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from AttackDetectionModel.ipynb\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install pandas\n",
    "# !pip3 install seaborn\n",
    "# !pip3 install --upgrade tensorflow-gpu\n",
    "# !pip3 install import-ipynb\n",
    "# !pip3 install cuda-python\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import import_ipynb\n",
    "import AttackDetectionModel\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3358dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction = 0.333)\n",
    "sess = tf.compat.v1.Session(config = tf.compat.v1.ConfigProto(gpu_options = gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04791b",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ff3684",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392f5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign, df_attack = AttackDetectionModel.get_files_botiot(\"../../botiot\")\n",
    "\n",
    "df_benign = np.concatenate([df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, \n",
    "                            df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, \n",
    "                            df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, \n",
    "                            df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign,\n",
    "                            df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign, df_benign])\n",
    "df_benign = shuffle(df_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb0bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize with the min-max scaler\n",
    "scaler = MinMaxScaler()\n",
    "df_benign_norm = scaler.fit_transform(df_benign)\n",
    "df_attack_norm = scaler.fit_transform(df_attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f03749",
   "metadata": {},
   "source": [
    "# Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3237bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "len_benign_train = int(0.7 * len(df_benign_norm))\n",
    "X_train = df_benign_norm[:len_benign_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26ae71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test_benign = df_benign_norm[len_benign_train:]\n",
    "X_test = np.concatenate([X_test_benign, df_attack_norm])\n",
    "\n",
    "Y_test = np.ones(len(X_test))\n",
    "Y_test[:len(X_test_benign)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ee87d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a8a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "original_dim = X_train.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)\n",
    "\n",
    "# Initial values\n",
    "epochs = 5\n",
    "learning_rate = 0.0001\n",
    "batch_size = 10\n",
    "anomaly_threshold = 0.05\n",
    "\n",
    "# Dictionary\n",
    "dict_params = { 'learning_rate': learning_rate, 'batch_size': round(batch_size), 'epochs': round(epochs)}\n",
    "pbounds = { 'learning_rate': (0.000001, 0.001), 'batch_size': (10, 100), 'epochs': (50, 1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c66ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Loss function\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # Compute the average MSE error, then scale it up (sum on all axes)\n",
    "    \n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))\n",
    "    \n",
    "    # Compute the KL loss\n",
    "    \n",
    "    kl_loss = - 0.5 * K.sum(1 + z_var - K.square(z_mean) - K.square(K.exp(z_var)), axis=-1)\n",
    "    \n",
    "    # Return the average loss over all \n",
    "    \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss) # Total_loss = reconstruction_loss + kl_loss \n",
    "    return total_loss\n",
    "\n",
    "# (1) Reconstruction Loss - Forces the encoder to generate latent features that minimize the reconstruction error, or else is\n",
    "# penalized\n",
    "# (2) KL Loss - Forces the distribution generated by the encoder to be similar to the prior probability of the input vector, \n",
    "# pushing latent feature space to normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72309d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 35)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 17)           612         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 11)           198         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_var (Dense)                  (None, 11)           198         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 11)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_var[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,008\n",
      "Trainable params: 1,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "inputs, encoder, z_var, z_mean = AttackDetectionModel.vae_encoder(input_shape, \n",
    "                                                                  intermediate_dim, \n",
    "                                                                  latent_dim, \n",
    "                                                                  AttackDetectionModel.reparametrization)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9cf7f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 11)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17)                204       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 35)                630       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 834\n",
      "Trainable params: 834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "decoder = AttackDetectionModel.vae_decoder(intermediate_dim, latent_dim, original_dim)\n",
    "outputs = decoder(encoder(inputs))\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5dee9",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3147d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_training(X_train = X_train, \n",
    "                      X_test = X_test, \n",
    "                      Y_test = Y_test, \n",
    "                      inputs = inputs, \n",
    "                      outputs = outputs, \n",
    "                      vae_loss = vae_loss,\n",
    "                      learning_rate = learning_rate,\n",
    "                      batch_size = batch_size,\n",
    "                      epochs = epochs,\n",
    "                      anomaly_threshold = anomaly_threshold):    \n",
    "    \n",
    "    # Create model\n",
    "    opt_adam = optimizers.Adam(learning_rate = dict_params['learning_rate'], clipvalue = 0.5)\n",
    "    model = Model(inputs, outputs, name = 'vae_mlp')\n",
    "    model.compile(optimizer = opt_adam, loss = vae_loss)\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(X_train, \n",
    "                        X_train, \n",
    "                        shuffle = True, \n",
    "                        verbose = 0,\n",
    "                        epochs = dict_params['epochs'], \n",
    "                        batch_size = dict_params['batch_size'])\n",
    "    \n",
    "    # Maximize the f1-score\n",
    "    X_pred_opt = model.predict(X_test)\n",
    "    error_vector_opt = AttackDetectionModel.get_error_term(X_pred_opt, X_test, _rmse = False)\n",
    "    Y_pred_opt = (error_vector_opt > anomaly_threshold)\n",
    "    f1 = f1_score(Y_test, Y_pred_opt)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63435bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Apply Bayesian optimization to choose the best hyperparameters\\n\\nopt = BayesianOptimization(f = maximize_training,\\n                           pbounds = pbounds,\\n                           verbose = 2, \\n                           random_state = 1)\\n\\nopt_start = time.time()\\n\\nopt.maximize(init_points = 5, n_iter = 5)\\n\\nopt_end = time.time()\\nopt_time = opt_end - opt_start\\nprint(\"Optimization time:\", opt_time)\\n\\nlearning_rate = opt.max[\\'params\\'][\\'learning_rate\\']\\nepochs = round(opt.max[\\'params\\'][\\'epochs\\'])\\nbatch_size = round(opt.max[\\'params\\'][\\'batch_size\\'])\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Apply Bayesian optimization to choose the best hyperparameters\n",
    "\n",
    "opt = BayesianOptimization(f = maximize_training,\n",
    "                           pbounds = pbounds,\n",
    "                           verbose = 2, \n",
    "                           random_state = 1)\n",
    "\n",
    "opt_start = time.time()\n",
    "\n",
    "opt.maximize(init_points = 5, n_iter = 5)\n",
    "\n",
    "opt_end = time.time()\n",
    "opt_time = opt_end - opt_start\n",
    "print(\"Optimization time:\", opt_time)\n",
    "\n",
    "learning_rate = opt.max['params']['learning_rate']\n",
    "epochs = round(opt.max['params']['epochs'])\n",
    "batch_size = round(opt.max['params']['batch_size'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82a597",
   "metadata": {},
   "source": [
    "# Predict Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "703c2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step with the best hyperparameters\n",
    "learning_rate = 0.0005007978127917845\n",
    "epochs = 550\n",
    "batch_size = 19\n",
    "opt_time = 1151.2620151042938\n",
    "anomaly_threshold = 0.043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46d064a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Train on 15025 samples\n",
      "Epoch 1/550\n",
      "15025/15025 [==============================] - 2s 102us/sample - loss: 40.3370\n",
      "Epoch 2/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 14.0464\n",
      "Epoch 3/550\n",
      "15025/15025 [==============================] - 1s 72us/sample - loss: 11.7292\n",
      "Epoch 4/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 10.9063\n",
      "Epoch 5/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 10.1932\n",
      "Epoch 6/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 9.6344\n",
      "Epoch 7/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 9.2910\n",
      "Epoch 8/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 8.9455\n",
      "Epoch 9/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 8.5871\n",
      "Epoch 10/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 8.3938\n",
      "Epoch 11/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 8.1582\n",
      "Epoch 12/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.8513\n",
      "Epoch 13/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.7380\n",
      "Epoch 14/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.5635\n",
      "Epoch 15/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.5354\n",
      "Epoch 16/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.4239\n",
      "Epoch 17/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.3722\n",
      "Epoch 18/550\n",
      "15025/15025 [==============================] - 1s 72us/sample - loss: 7.2776\n",
      "Epoch 19/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.2319\n",
      "Epoch 20/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.1421\n",
      "Epoch 21/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 7.0641\n",
      "Epoch 22/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 7.0165\n",
      "Epoch 23/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.9810\n",
      "Epoch 24/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.9515\n",
      "Epoch 25/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.8916\n",
      "Epoch 26/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.8336\n",
      "Epoch 27/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.8144\n",
      "Epoch 28/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.7942\n",
      "Epoch 29/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.7544\n",
      "Epoch 30/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.6757\n",
      "Epoch 31/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.6697\n",
      "Epoch 32/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.6219\n",
      "Epoch 33/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.5810\n",
      "Epoch 34/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.5072\n",
      "Epoch 35/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.5149\n",
      "Epoch 36/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.4322\n",
      "Epoch 37/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.4395\n",
      "Epoch 38/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.4165\n",
      "Epoch 39/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.3672\n",
      "Epoch 40/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.3601\n",
      "Epoch 41/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.2974\n",
      "Epoch 42/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.3043\n",
      "Epoch 43/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.2514\n",
      "Epoch 44/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.2941\n",
      "Epoch 45/550\n",
      "15025/15025 [==============================] - 1s 72us/sample - loss: 6.2340\n",
      "Epoch 46/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.2278\n",
      "Epoch 47/550\n",
      "15025/15025 [==============================] - 1s 72us/sample - loss: 6.2199\n",
      "Epoch 48/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.2035\n",
      "Epoch 49/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.1725\n",
      "Epoch 50/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.1409\n",
      "Epoch 51/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.1142\n",
      "Epoch 52/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.1335\n",
      "Epoch 53/550\n",
      "15025/15025 [==============================] - 1s 75us/sample - loss: 6.1312\n",
      "Epoch 54/550\n",
      "15025/15025 [==============================] - 1s 73us/sample - loss: 6.1100\n",
      "Epoch 55/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.0887\n",
      "Epoch 56/550\n",
      "15025/15025 [==============================] - 1s 74us/sample - loss: 6.1399\n",
      "Epoch 57/550\n",
      "   19/15025 [..............................] - ETA: 1s - loss: 4.9362"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m vae_model, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mAttackDetectionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set the optimized anomaly threshold\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#anomaly_threshold = AttackDetectionModel.get_anomaly_threshold(X_train, vae_model)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[1;32m     11\u001b[0m X_pred, test_time \u001b[38;5;241m=\u001b[39m AttackDetectionModel\u001b[38;5;241m.\u001b[39mtest(X_test, vae_model)\n",
      "File \u001b[0;32m<string>:5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_train, inputs, outputs, vae_loss, learning_rate, epochs, batch_size)\u001b[0m\n",
      "File \u001b[0;32m<string>:9\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(X_train, inputs, outputs, vae_loss, learning_rate, epochs, batch_size)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/training_v1.py:854\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    853\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/training_arrays_v1.py:431\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m aggregator\u001b[38;5;241m.\u001b[39maggregate(batch_outs, batch_start, batch_end)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Callbacks batch end.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m batch_logs \u001b[38;5;241m=\u001b[39m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_logs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_logs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_outs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_index, batch_logs)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/callbacks.py:619\u001b[0m, in \u001b[0;36mCallbackList.make_logs\u001b[0;34m(self, model, logs, outputs, mode, prefix)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n\u001b[0;32m--> 619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/callbacks.py:190\u001b[0m, in \u001b[0;36mmake_logs\u001b[0;34m(model, logs, outputs, mode, prefix)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_logs\u001b[39m(model, logs, outputs, mode, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes logs for sending to `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     metric_names \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_names\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m {ModeKeys\u001b[38;5;241m.\u001b[39mTRAIN, ModeKeys\u001b[38;5;241m.\u001b[39mTEST} \u001b[38;5;129;01mand\u001b[39;00m metric_names:\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(metric_names, outputs):\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/training_v1.py:576\u001b[0m, in \u001b[0;36mModel.metrics_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m         metrics_names\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    568\u001b[0m             [\n\u001b[1;32m    569\u001b[0m                 e\u001b[38;5;241m.\u001b[39mloss_name()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m             ]\n\u001b[1;32m    573\u001b[0m         )\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Add all metric names.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m metrics_names \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m]\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics_names\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/training_v1.py:546\u001b[0m, in \u001b[0;36mModel.metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    542\u001b[0m     metrics \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_metric_functions\n\u001b[1;32m    543\u001b[0m metrics\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics)\n\u001b[1;32m    544\u001b[0m metrics\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m    545\u001b[0m     _get_metrics_from_layers(\n\u001b[0;32m--> 546\u001b[0m         \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flatten_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_self\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/base_layer.py:3228\u001b[0m, in \u001b[0;36mLayer._flatten_layers\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flatten_layers\u001b[39m(\u001b[38;5;28mself\u001b[39m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_self\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m-> 3228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_modules(\n\u001b[1;32m   3229\u001b[0m         recursive\u001b[38;5;241m=\u001b[39mrecursive, include_self\u001b[38;5;241m=\u001b[39minclude_self\n\u001b[1;32m   3230\u001b[0m     ):\n\u001b[1;32m   3231\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Layer):\n\u001b[1;32m   3232\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/keras/engine/base_layer.py:3260\u001b[0m, in \u001b[0;36mLayer._flatten_modules\u001b[0;34m(self, recursive, include_self)\u001b[0m\n\u001b[1;32m   3257\u001b[0m seen_object_ids\u001b[38;5;241m.\u001b[39madd(trackable_id)\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# Metrics are not considered part of the Layer's topology.\u001b[39;00m\n\u001b[0;32m-> 3260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trackable_obj, tf\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrackable_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMetric\u001b[49m\n\u001b[1;32m   3262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m trackable_obj\n\u001b[1;32m   3264\u001b[0m     \u001b[38;5;66;03m# Introspect recursively through sublayers.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/abc.py:119\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_abc_instancecheck\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(\"Iteration \" + str(i))\n",
    "    \n",
    "    # Train\n",
    "    vae_model, train_time = AttackDetectionModel.train(X_train, inputs, outputs, vae_loss, learning_rate, epochs, batch_size)\n",
    "    \n",
    "    # Set the optimized anomaly threshold\n",
    "    #anomaly_threshold = AttackDetectionModel.get_anomaly_threshold(X_train, vae_model)\n",
    "    \n",
    "    # Test\n",
    "    X_pred, test_time = AttackDetectionModel.test(X_test, vae_model)\n",
    "    Y_test, Y_pred = AttackDetectionModel.get_prediction(Y_test, X_pred, X_test, anomaly_threshold, vae_model)\n",
    "    \n",
    "    # Metrics\n",
    "    acc, f1, pre, rec = AttackDetectionModel.get_scores(Y_test, Y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    AttackDetectionModel.print_results(number_features,\n",
    "                                       learning_rate,\n",
    "                                       epochs,\n",
    "                                       batch_size,\n",
    "                                       anomaly_threshold,\n",
    "                                       X_train,\n",
    "                                       X_test,\n",
    "                                       opt_time,\n",
    "                                       train_time,\n",
    "                                       test_time,\n",
    "                                       acc,\n",
    "                                       f1,\n",
    "                                       pre,\n",
    "                                       rec,\n",
    "                                       Y_test,\n",
    "                                       Y_pred,\n",
    "                                       \"Results/botiot.txt\")\n",
    "    \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84108b",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examinig the latent space generated by the encoder\n",
    "X_encoded = encoder.predict(X_test)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_analysis = pca.fit_transform(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4518c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x = X_analysis[:, 0], y = X_analysis[:, 1], s = 20, hue = Y_pred)\n",
    "plt.grid()\n",
    "plt.savefig(\"Results/PCA/Botiot\")\n",
    "plt.show()\n",
    "\n",
    "# Orange ones are anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d1451",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "sns.scatterplot(x = X_analysis[:, 0], y = X_analysis[:, 1], s = 20, hue = Y_test)\n",
    "plt.grid()\n",
    "plt.savefig(\"Results/PCA Groundtruth/Botiot\")\n",
    "plt.show()\n",
    "\n",
    "# Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (5, 5))\n",
    "ax.matshow(conf_matrix, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x = j, y = i,s = conf_matrix[i, j], va = 'center', ha = 'center', size = 'large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=12)\n",
    "plt.ylabel('Actuals', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.savefig(\"Results/Confusion Matrix/Botiot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6750e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
